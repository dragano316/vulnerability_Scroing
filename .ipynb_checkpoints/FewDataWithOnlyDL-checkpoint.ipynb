{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "opposite-samba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import io\n",
    "df_1=pd.read_csv('cve_cvss_scores.csv',error_bad_lines=False,sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "invisible-suspect",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "revised-lindsay",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cubic-network",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1['Description'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "assured-sarah",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "proved-lease",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2 = df_1.dropna()\n",
    "df_2.drop(['Published Date','Last Modified Date'],axis='columns',inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "major-institute",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "recent-malta",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_3 = df_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "norwegian-jacket",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_3 = pd.get_dummies(df_2, columns=[\"Attack Complexity\",\"Attack Vector\",\"Availability Impact\",\"Confidentiality Impact\",\"Integrity Impact\",\"Privileges Required\",\"Scope\",\"UserInteraction\"])\n",
    "df_3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "opponent-painting",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intermediate-times",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_4 = pd.get_dummies(df_3, columns=[\"base Severity\",\"Access Complexity\",\"Access Vector\",\"Authentication\",\"Availability Impact.1\"])\n",
    "df_4.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "timely-briefs",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_5 = df_4.drop(['Obtain All Privilege','Obtain Other Privilege','Obtain User Privilege','Vector String.1'],axis='columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "previous-tender",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_5.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spanish-covering",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_6 = pd.get_dummies(df_5, columns=[\"User Interaction Required\",\"Integrity Impact.1\",\"Confidentiality Impact.1\"])\n",
    "df_6.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accomplished-postcard",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_7 = df_6.drop(['severity'],axis='columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "convenient-texas",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_8 = df_7.drop(['CVE','Vector String'],axis='columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cutting-torture",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_data = df_8['baseScore']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "living-seattle",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = df_8.drop(['baseScore'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "combined-hamilton",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(input_data, output_data, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "comparable-execution",
   "metadata": {},
   "outputs": [],
   "source": [
    "des_train = X_train['Description']\n",
    "des_test = X_test['Description']\n",
    "X_train = X_train.drop(['Description'],axis=1)\n",
    "X_test = X_test.drop(['Description'],axis=1)\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "modular-object",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd # our main data management package\n",
    "# import matplotlib.pyplot as plt \n",
    "# our main display package\n",
    "import string # used for preprocessing\n",
    "import re # used for preprocessing\n",
    "import nltk # the Natural Language Toolkit, used for preprocessing\n",
    "import numpy as np # used for managing NaNs\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords # used for preprocessing\n",
    "from nltk.stem import WordNetLemmatizer # used for preprocessing\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression # our model\n",
    "from sklearn.model_selection import train_test_split\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "failing-delight",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove urls, handles, and the hashtag from hashtags (taken from https://stackoverflow.com/questions/8376691/how-to-remove-hashtag-user-link-of-a-tweet-using-regular-expression)\n",
    "def remove_urls(text):\n",
    "    new_text = ' '.join(re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\",\" \",text).split())\n",
    "    return new_text\n",
    "# make all text lowercase\n",
    "def text_lowercase(text):\n",
    "    return text.lower()\n",
    "# remove numbers\n",
    "def remove_numbers(text):\n",
    "    result = re.sub(r'\\d+', '', text)\n",
    "    return result\n",
    "# remove punctuation\n",
    "def remove_punctuation(text):\n",
    "    translator = str.maketrans('', '', string.punctuation)\n",
    "    return text.translate(translator)\n",
    "# tokenize\n",
    "def tokenize(text):\n",
    "    text = word_tokenize(text)\n",
    "    return text\n",
    "# remove stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "def remove_stopwords(text):\n",
    "    text = [i for i in text if not i in stop_words]\n",
    "    return text\n",
    "# lemmatize\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "def lemmatize(text):\n",
    "    text = [lemmatizer.lemmatize(token) for token in text]\n",
    "    return text\n",
    "\n",
    "def preprocessing(text):\n",
    "    text = text_lowercase(text)\n",
    "    text = remove_urls(text)\n",
    "    text = remove_numbers(text)\n",
    "    text = remove_punctuation(text)\n",
    "    text = tokenize(text)\n",
    "    text = remove_stopwords(text)\n",
    "    text = lemmatize(text)\n",
    "    text = ' '.join(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "olive-spouse",
   "metadata": {},
   "outputs": [],
   "source": [
    "pp_text_train = [] # our preprocessed text column\n",
    "df_10 = pd.DataFrame()\n",
    "# for text_data in X_train_2:\n",
    "for text_data in des_train:\n",
    "    pp_text_data = preprocessing(text_data)\n",
    "    pp_text_train.append(pp_text_data)\n",
    "df_10['pp_text'] = pp_text_train # add the preprocessed text as a column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stretch-charity",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_10.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "residential-roller",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_10.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "muslim-proceeding",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_10['output'] = y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "polyphonic-generic",
   "metadata": {},
   "outputs": [],
   "source": [
    "pp_text_test = [] # our preprocessed text column\n",
    "df_10_test = pd.DataFrame()\n",
    "for text_data in des_test:\n",
    "    pp_text_data = preprocessing(text_data)\n",
    "    pp_text_test.append(pp_text_data)\n",
    "df_10_test['pp_text'] = pp_text_test # add the preprocessed text as a column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "animated-afternoon",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_10_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "impressed-fourth",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text_data = list(df_10['pp_text'])\n",
    "test_text_data = list(df_10_test['pp_text'])\n",
    "corpus = train_text_data + test_text_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eight-batch",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf=TfidfVectorizer()\n",
    "# the vectorizer must be fit onto the entire corpus\n",
    "fitted_vectorizer = tf.fit(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "increased-headquarters",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train\n",
    "train_transform = fitted_vectorizer.transform(df_10['pp_text'])\n",
    "# y = train['target']\n",
    "# test\n",
    "test_transform = fitted_vectorizer.transform(df_10_test['pp_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "national-midwest",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "developed-monitoring",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier=Sequential()\n",
    "classifier.add(Dense(units=10,activation='relu',input_dim=51))\n",
    "classifier.add(Dropout(0.2))\n",
    "classifier.add(Dense(units=10,activation='relu'))\n",
    "classifier.add(Dropout(0.2))\n",
    "classifier.add(Dense(units=1))\n",
    "classifier.compile(optimizer='adam',loss='mse',metrics=['mse'])\n",
    "classifier.fit(X_train,y_train,batch_size=32,epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fundamental-jason",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred=classifier.predict(X_test)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "heard-variety",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report,confusion_matrix,accuracy_score,mean_squared_error\n",
    "m_s_e_3 = mean_squared_error(y_test,y_pred)\n",
    "print(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "transsexual-bedroom",
   "metadata": {},
   "outputs": [],
   "source": [
    "m_s_e_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spoken-benjamin",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "steady-examination",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_transform.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "noble-concept",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier1=Sequential()\n",
    "classifier1.add(Dense(units=6,activation='relu',input_dim=47119))\n",
    "classifier1.add(Dropout(0.2))\n",
    "classifier1.add(Dense(units=6,activation='relu'))\n",
    "classifier1.add(Dropout(0.2))\n",
    "classifier1.add(Dense(units=1))\n",
    "classifier1.compile(optimizer='adam',loss='mse',metrics=['mse'])\n",
    "classifier1.fit(train_transform.astype(np.uint8).toarray(),y_train,batch_size=32,epochs=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "joint-pizza",
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_des_pred=classifier1.predict(test_transform)\n",
    "testing_des_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "behind-sweden",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report,confusion_matrix,accuracy_score,mean_squared_error\n",
    "m_s_e_4 = mean_squared_error(y_test,testing_des_pred)\n",
    "print(m_s_e_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "critical-equipment",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
